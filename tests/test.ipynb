{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the depth values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import cv2\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=\"/home/foxy_user/foxy_ws/src/gail_navigation/GailNavigationNetwork/data/traj2.hdf5\"\n",
    "read_file= h5py.File(file_path, \"r\")\n",
    "print(f\"list of kesy {list(read_file.keys())}\")\n",
    "len= read_file['kris_dynamics']['odom_data']['target_vector'].shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class depthcheker():\n",
    "    def __init__(self,color_img,depth_img):\n",
    "        \n",
    "            self.image = color_img  # Change to your image path\n",
    "            self.depth = depth_img\n",
    "            self.old_img = copy.deepcopy(self.image)\n",
    "          # Change to your depth map path\n",
    "\n",
    "            # Create a window and display the image\n",
    "            cv2.namedWindow(\"Image\")\n",
    "            cv2.imshow(\"Image\", self.old_img)\n",
    "\n",
    "            # Set mouse callback to show depth value\n",
    "            cv2.setMouseCallback(\"Image\", self.show_depth)\n",
    "            while True:\n",
    "              key = cv2.waitKey(1) & 0xFF\n",
    "              if key == ord(\"q\"):  # Press 'q' to exit\n",
    "                  break\n",
    "\n",
    "            # cv2.waitKey(0)\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "    def show_depth(self,event, x, y, flags, param):\n",
    "        if event == cv2.EVENT_LBUTTONDOWN:\n",
    "            depth_value = self.depth[y, x]\n",
    "            temp_img = copy.deepcopy(self.image)\n",
    "            cv2.putText(temp_img, \"Depth: {:.2f}\".format(depth_value), (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "            cv2.imshow(\"Image\",temp_img)\n",
    "            print(f\"depth value {depth_value} at x {x} y {y}\")\n",
    "\n",
    "\n",
    "    # Read the image and depth map\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_depth=True\n",
    "for i in range(len):\n",
    "    target=read_file['kris_dynamics']['odom_data']['target_vector'][i]\n",
    "    rgb=read_file['images']['rgb_data'][i]\n",
    "    depth=read_file['images']['depth_data'][i]\n",
    "    act=read_file['kris_dynamics']['odom_data']['odom_data_wheel'][i] \n",
    "    if test_depth:\n",
    "        check_depth=depthcheker(rgb,depth)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing flattening the space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import spaces\n",
    "import numpy as np                                                                                                                                                                                                                                                                                                                                                                                          \n",
    "from gymnasium.spaces.utils import flatten_space, flatten\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178091,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_space = spaces.Dict({\n",
    "            'target_vector': spaces.Box(low=-100.0, high=100.0, shape=(1,7), dtype=np.float32),\n",
    "            'rgb_features': spaces.Box(low=-np.inf, high=np.inf, shape=(1280, 8, 10), dtype=np.float32),\n",
    "            'depth_features': spaces.Box(low=-np.inf, high=np.inf, shape=(238,318), dtype=np.float32)\n",
    "        })\n",
    "\n",
    "flatten(observation_space, observation_space.sample()).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating trajectories version 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from imitation.policies.serialize import load_policy\n",
    "from imitation.util.util import make_vec_env\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from imitation.data import rollout\n",
    "from imitation.algorithms.adversarial.gail import GAIL\n",
    "from imitation.rewards.reward_nets import BasicRewardNet\n",
    "from imitation.util.networks import RunningNorm\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from imitation.data.types import Trajectory,DictObs,TrajectoryWithRew,Tuple\n",
    "import h5py\n",
    "from GailNavigationNetwork.model import NaviNet\n",
    "from GailNavigationNetwork.utilities import preprocess\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def create_demos(file_path,DEVICE=\"cuda\"):\n",
    "    '''\n",
    "    Creates a gymnasium transition from the given file path\n",
    "    of hdf5 file of known structure\n",
    "\n",
    "    Args:\n",
    "    file_path: str  \n",
    "    Path to the hdf5 file   \n",
    "\n",
    "    Returns:\n",
    "    rollouts: gymnasium.Transition\n",
    "\n",
    "    '''    \n",
    "    read_file= h5py.File(file_path, \"r\")\n",
    "    model= NaviNet().to(DEVICE)\n",
    "    model.eval()\n",
    "    len= read_file['kris_dynamics']['odom_data']['target_vector'].shape[0]\n",
    "    rgbs=[]\n",
    "    depths=[]\n",
    "    targets=[]  \n",
    "    acts=[]\n",
    "    ob=[]\n",
    "    for i in range(len):\n",
    "        target=read_file['kris_dynamics']['odom_data']['target_vector'][i]\n",
    "        rgb=read_file['images']['rgb_data'][i]\n",
    "        depth=read_file['images']['depth_data'][i]\n",
    "        act=read_file['kris_dynamics']['odom_data']['odom_data_wheel'][i]\n",
    "        # print(f\"depth shape in rollout {depth.shape}\")\n",
    "        rgb=preprocess(rgb)\n",
    "        depth=preprocess(depth)\n",
    "        # rgb,depth=preprocess(rgb,depth)\n",
    "        (rgb, depth) = (rgb.to(DEVICE), depth.to(DEVICE))\n",
    "        rgb_features, depth_features = model(rgb,depth)\n",
    "        rgb_features=rgb_features.detach().cpu().numpy()\n",
    "        depth_features=depth_features.detach().cpu().numpy()\n",
    "        rgbs.append(rgb_features)\n",
    "        depths.append(depth_features)\n",
    "        targets.append(target) \n",
    "        acts.append(act)\n",
    "        \n",
    "\n",
    "    acts=np.array(acts[:-1])\n",
    "    dones=[False for i in range(len)]\n",
    "    dones[-1]=True\n",
    "    infos= [{} for i in range(len-1)]\n",
    "    rgbs=np.array(rgbs).reshape(25, 1, 1280, 8, 10)\n",
    "    depths=np.array(depths).reshape(25, 1,  238, 318)\n",
    "    targets=np.array(targets).reshape(25, 1, 7)\n",
    "\n",
    "\n",
    "\n",
    "# Combine into one tuple\n",
    "\n",
    "    # obs_dict=tuple((targets,\n",
    "    #         rgbs,\n",
    "    #          depths))\n",
    "    # for i in range(len-1):\n",
    "    #     obs_dict[0][i]=np.array(obs_dict[0][i])\n",
    "    #     obs_dict[1][i]=np.array(obs_dict[1][i])\n",
    "    #     obs_dict[2][i]=np.array(obs_dict[2][i])\n",
    "    \n",
    "    # print(f\"obs_dict shape {obs_tuple[0].shape} {obs_tuple[1].shape} {obs_tuple[2].shape}\")\n",
    "    ob=np.array(ob)\n",
    "\n",
    "    traj = Trajectory(obs=ob, acts=acts,infos=infos,terminal=dones)\n",
    "\n",
    "\n",
    "    return rollout.flatten_trajectories([traj])\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "file_path=\"/home/foxy_user/foxy_ws/src/gail_navigation/GailNavigationNetwork/data/traj2.hdf5\"\n",
    "demonstrations,rollout=create_demos(file_path)\n",
    "print(f\"rollout {rollout[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating trajectories version 2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import collections\n",
    "from typing import (\n",
    "    Any,    \n",
    "    Dict,\n",
    "    Hashable, \n",
    "    List,\n",
    "    Mapping,    \n",
    "    Sequence,    \n",
    "    Union,\n",
    ")\n",
    "\n",
    "from GailNavigationNetwork.model import NaviNet\n",
    "from GailNavigationNetwork.utilities import preprocess\n",
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3.common.base_class import BaseAlgorithm\n",
    "from stable_baselines3.common.policies import BasePolicy\n",
    "from stable_baselines3.common.utils import check_for_correct_spaces\n",
    "from stable_baselines3.common.vec_env import VecEnv\n",
    "from imitation.data import types\n",
    "import h5py\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "class TrajectoryAccumulator:\n",
    "    \"\"\"Accumulates trajectories step-by-step.\n",
    "\n",
    "    Useful for collecting completed trajectories while ignoring partially-completed\n",
    "    trajectories (e.g. when rolling out a VecEnv to collect a set number of\n",
    "    transitions). Each in-progress trajectory is identified by a 'key', which enables\n",
    "    several independent trajectories to be collected at once. They key can also be left\n",
    "    at its default value of `None` if you only wish to collect one trajectory.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialise the trajectory accumulator.\"\"\"\n",
    "        self.partial_trajectories = collections.defaultdict(list)\n",
    "\n",
    "    def add_step(\n",
    "        self,\n",
    "        step_dict: Mapping[str, Union[types.Observation, Mapping[str, Any]]],\n",
    "        key: Hashable = None,\n",
    "    ) -> None:\n",
    "        \"\"\"Add a single step to the partial trajectory identified by `key`.\n",
    "\n",
    "        Generally a single step could correspond to, e.g., one environment managed\n",
    "        by a VecEnv.\n",
    "\n",
    "        Args:\n",
    "            step_dict: dictionary containing information for the current step. Its\n",
    "                keys could include any (or all) attributes of a `TrajectoryWithRew`\n",
    "                (e.g. \"obs\", \"acts\", etc.).\n",
    "            key: key to uniquely identify the trajectory to append to, if working\n",
    "                with multiple partial trajectories.\n",
    "        \"\"\"\n",
    "        print(f\"added partial traj\")\n",
    "        self.partial_trajectories[key].append(step_dict)\n",
    "\n",
    "    def finish_trajectory(\n",
    "        self,\n",
    "        key: Hashable,\n",
    "        terminal: bool,\n",
    "    ) -> types.TrajectoryWithRew:\n",
    "        \"\"\"Complete the trajectory labelled with `key`.\n",
    "\n",
    "        Args:\n",
    "            key: key uniquely identifying which in-progress trajectory to remove.\n",
    "            terminal: trajectory has naturally finished (i.e. includes terminal state).\n",
    "\n",
    "        Returns:\n",
    "            traj: list of completed trajectories popped from\n",
    "                `self.partial_trajectories`.\n",
    "        \"\"\"\n",
    "        part_dicts = self.partial_trajectories[key]\n",
    "        del self.partial_trajectories[key]\n",
    "        out_dict_unstacked = collections.defaultdict(list)\n",
    "        for part_dict in part_dicts:\n",
    "            for k, array in part_dict.items():\n",
    "                out_dict_unstacked[k].append(array)\n",
    "\n",
    "        out_dict_stacked = {\n",
    "            k: types.stack_maybe_dictobs(arr_list)\n",
    "            for k, arr_list in out_dict_unstacked.items()\n",
    "        }\n",
    "        traj = types.TrajectoryWithRew(**out_dict_stacked, terminal=terminal)\n",
    "        assert traj.rews.shape[0] == traj.acts.shape[0] == len(traj.obs) - 1\n",
    "        return traj\n",
    "\n",
    "    def add_steps_and_auto_finish(\n",
    "        self,\n",
    "        acts: np.ndarray,\n",
    "        obs: Union[types.Observation, Dict[str, np.ndarray]],\n",
    "        rews: np.ndarray,\n",
    "        dones: np.ndarray,\n",
    "        infos: List[dict],\n",
    "    ) -> List[types.TrajectoryWithRew]:\n",
    "        \"\"\"Calls `add_step` repeatedly using acts and the returns from `venv.step`.\n",
    "\n",
    "        Also automatically calls `finish_trajectory()` for each `done == True`.\n",
    "        Before calling this method, each environment index key needs to be\n",
    "        initialized with the initial observation (usually from `venv.reset()`).\n",
    "\n",
    "        See the body of `util.rollout.generate_trajectory` for an example.\n",
    "\n",
    "        Args:\n",
    "            acts: Actions passed into `VecEnv.step()`.\n",
    "            obs: Return value from `VecEnv.step(acts)`.\n",
    "            rews: Return value from `VecEnv.step(acts)`.\n",
    "            dones: Return value from `VecEnv.step(acts)`.\n",
    "            infos: Return value from `VecEnv.step(acts)`.\n",
    "\n",
    "        Returns:\n",
    "            A list of completed trajectories. There should be one trajectory for\n",
    "            each `True` in the `dones` argument.\n",
    "        \"\"\"\n",
    "        trajs: List[types.TrajectoryWithRew] = []\n",
    "        wrapped_obs = types.maybe_wrap_in_dictobs(obs)\n",
    "\n",
    "        # iterate through environments\n",
    "        for env_idx in range(len(wrapped_obs)):\n",
    "            assert env_idx in self.partial_trajectories\n",
    "            assert list(self.partial_trajectories[env_idx][0].keys()) == [\"obs\"], (\n",
    "                \"Need to first initialize partial trajectory using \"\n",
    "                \"self._traj_accum.add_step({'obs': ob}, key=env_idx)\"\n",
    "            )\n",
    "        print(f'Debug : {acts} ')\n",
    "        print(f'Debug : {len(wrapped_obs)} ')\n",
    "\n",
    "        print(f'Debug : {rews} ')\n",
    "        print(f'Debug :  {dones}')\n",
    "        print(f'Debug :  {len(infos)}')\n",
    "\n",
    "\n",
    "\n",
    "        # iterate through steps\n",
    "        zip_iter = enumerate(zip(acts, wrapped_obs, rews, dones, infos))\n",
    "        for env_idx, (act, ob, rew, done, info) in zip_iter:\n",
    "            if done:\n",
    "                # When dones[i] from VecEnv.step() is True, obs[i] is the first\n",
    "                # observation following reset() of the ith VecEnv, and\n",
    "                # infos[i][\"terminal_observation\"] is the actual final observation.\n",
    "                real_ob = types.maybe_wrap_in_dictobs(info[\"terminal_observation\"])\n",
    "            else:\n",
    "                real_ob = ob\n",
    "\n",
    "            self.add_step(\n",
    "                dict(\n",
    "                    acts=act,\n",
    "                    rews=rew,\n",
    "                    # this is not the obs corresponding to `act`, but rather the obs\n",
    "                    # *after* `act` (see above)\n",
    "                    obs=real_ob,\n",
    "                    infos=info,\n",
    "                ),\n",
    "                env_idx,\n",
    "            )\n",
    "            if done:\n",
    "                # finish env_idx-th trajectory\n",
    "                new_traj = self.finish_trajectory(env_idx, terminal=True)\n",
    "                trajs.append(new_traj)\n",
    "                # When done[i] from VecEnv.step() is True, obs[i] is the first\n",
    "                # observation following reset() of the ith VecEnv.\n",
    "                self.add_step(dict(obs=ob), env_idx)\n",
    "        return trajs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TrajFromFile:\n",
    "\n",
    "    def __init__(self,file_path,DEVICE=\"cuda\"):\n",
    "        self.file_path=file_path\n",
    "        self.read_file= h5py.File(file_path, \"r\")\n",
    "        self.model= NaviNet().to(DEVICE)\n",
    "        self.model.eval()\n",
    "        self.len= self.read_file['kris_dynamics']['odom_data']['target_vector'].shape[0]\n",
    "        self.rgbs=[]\n",
    "        self.depths=[]\n",
    "        self.targets=[]  \n",
    "        self.acts=[]\n",
    "        self.ob=[]\n",
    "        self.DEVICE=DEVICE\n",
    "        ## Here get the dataset propetiees so features like sample_until can be used\n",
    "\n",
    "    def get_demo(self,idx):\n",
    "        ''''\n",
    "        Return the demonstration at the given index\n",
    "        '''\n",
    "        target=self.read_file['kris_dynamics']['odom_data']['target_vector'][idx]\n",
    "        rgb=self.read_file['images']['rgb_data'][idx]\n",
    "        depth=self.read_file['images']['depth_data'][idx]\n",
    "        act=self.read_file['kris_dynamics']['odom_data']['odom_data_wheel'][idx]\n",
    "        rgb=preprocess(rgb)\n",
    "        depth=preprocess(depth)\n",
    "        (rgb, depth) = (rgb.to(self.DEVICE), depth.to(self.DEVICE))\n",
    "        rgb_features, depth_features = self.model(rgb,depth)\n",
    "        rgb_features=rgb_features.detach().cpu().numpy()\n",
    "        depth_features=depth_features.detach().cpu().numpy()\n",
    "\n",
    "        obs={\n",
    "            'target_vector': np.array([target], dtype=np.float32),\n",
    "            'rgb_features': np.array([rgb_features], dtype=np.float32),\n",
    "            'depth_features': np.array([depth_features], dtype=np.float32)\n",
    "        }\n",
    "\n",
    "        rew=0\n",
    "        done=[False if self.len-1!=idx else True]\n",
    "        info={}\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        return act,obs,rew,done,info\n",
    "    \n",
    "    def generate_trajectories(\n",
    "        self,            \n",
    "        venv: VecEnv,\n",
    "        sample_until,     \n",
    "    ) -> Sequence[types.TrajectoryWithRew]:\n",
    "        \"\"\"Generate trajectory dictionaries from a policy and an environment.\n",
    "\n",
    "        Args:\n",
    "            policy: Can be any of the following:\n",
    "                1) A stable_baselines3 policy or algorithm trained on the gym environment.\n",
    "                2) A Callable that takes an ndarray of observations and returns an ndarray\n",
    "                of corresponding actions.\n",
    "                3) None, in which case actions will be sampled randomly.\n",
    "            venv: The vectorized environments to interact with.\n",
    "            sample_until: An integer defining lenth of recored trajectory\n",
    "            deterministic_policy: If True, asks policy to deterministically return\n",
    "                action. Note the trajectories might still be non-deterministic if the\n",
    "                environment has non-determinism!\n",
    "            rng: used for shuffling trajectories.\n",
    "\n",
    "        Returns:\n",
    "            Sequence of trajectories, satisfying `sample_until`. Additional trajectories\n",
    "            may be collected to avoid biasing process towards short episodes; the user\n",
    "            should truncate if required.\n",
    "        \"\"\"\n",
    "        # Replace this with the function to get the saved trjacotries\n",
    "        # But output in the same format as the policy_to_callable\n",
    "        # get_actions = policy_to_callable(policy, venv, deterministic_policy)\n",
    "\n",
    "        # Collect rollout tuples.\n",
    "        trajectories = []\n",
    "        # accumulator for incomplete trajectories\n",
    "        trajectories_accum = TrajectoryAccumulator()\n",
    "\n",
    "        # Initialize the trajectories with the first observation.\n",
    "        _, obs , _ , _ , _= self.get_demo(0)\n",
    "            # obs, rews, dones, infos = venv.step(acts)    \n",
    "\n",
    "        wrapped_obs = types.maybe_wrap_in_dictobs(obs)\n",
    "\n",
    "        # we use dictobs to iterate over the envs in a vecenv\n",
    "        for ob in (wrapped_obs):\n",
    "            # Seed with first obs only. Inside loop, we'll only add second obs from\n",
    "            # each (s,a,r,s') tuple, under the same \"obs\" key again. That way we still\n",
    "            # get all observations, but they're not duplicated into \"next obs\" and\n",
    "            # \"previous obs\" (this matters for, e.g., Atari, where observations are\n",
    "            # really big).\n",
    "            ## Here I replacec the env_idx with 1 as there is only a single idx \n",
    "            trajectories_accum.add_step(dict(obs=ob),0)\n",
    "            print(\"Added the first observation\")\n",
    "\n",
    "        # Now, we sample until `sample_until(trajectories)` is true.\n",
    "        # If we just stopped then this would introduce a bias towards shorter episodes,\n",
    "        # since longer episodes are more likely to still be active, i.e. in the process\n",
    "        # of being sampled from. To avoid this, we continue sampling until all epsiodes\n",
    "        # are complete.\n",
    "        #\n",
    "     \n",
    "        dones = np.zeros(venv.num_envs, dtype=bool)\n",
    "\n",
    "        \n",
    "        for idx in range(sample_until-1):\n",
    "            acts, obs , rews , dones , infos= self.get_demo(idx+1)\n",
    "            # obs, rews, dones, infos = venv.step(acts)    \n",
    "            wrapped_obs = types.maybe_wrap_in_dictobs(obs)\n",
    "\n",
    "            # If an environment is inactive, i.e. the episode completed for that\n",
    "            # environment after `sample_until(trajectories)` was true, then we do\n",
    "            # *not* want to add any subsequent trajectories from it. We avoid this\n",
    "            # by just making it never done.\n",
    "            # dones &= active\n",
    "\n",
    "            new_trajs = trajectories_accum.add_steps_and_auto_finish(\n",
    "                acts,\n",
    "                wrapped_obs,\n",
    "                rews,\n",
    "                dones,\n",
    "                infos,\n",
    "            )\n",
    "            trajectories.extend(new_trajs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Note that we just drop partial trajectories. This is not ideal for some\n",
    "        # algos; e.g. BC can probably benefit from partial trajectories, too.\n",
    "\n",
    "        # Each trajectory is sampled i.i.d.; however, shorter episodes are added to\n",
    "        # `trajectories` sooner. Shuffle to avoid bias in order. This is important\n",
    "        # when callees end up truncating the number of trajectories or transitions.\n",
    "        # It is also cheap, since we're just shuffling pointers.\n",
    "        # rng.shuffle(trajectories)  # type: ignore[arg-type]\n",
    "\n",
    "        # Sanity checks.\n",
    "        for trajectory in trajectories:\n",
    "            n_steps = len(trajectory.acts)\n",
    "            # extra 1 for the end\n",
    "            if isinstance(venv.observation_space, spaces.Dict):\n",
    "                exp_obs = {}\n",
    "                for k, v in venv.observation_space.items():\n",
    "                    assert v.shape is not None\n",
    "                    exp_obs[k] = (n_steps + 1,) + v.shape\n",
    "            else:\n",
    "                obs_space_shape = venv.observation_space.shape\n",
    "                assert obs_space_shape is not None\n",
    "                exp_obs = (n_steps + 1,) + obs_space_shape  # type: ignore[assignment]\n",
    "            real_obs = trajectory.obs.shape\n",
    "            assert real_obs == exp_obs, f\"expected shape {exp_obs}, got {real_obs}\"\n",
    "            assert venv.action_space.shape is not None\n",
    "            exp_act = (n_steps,) + venv.action_space.shape\n",
    "            real_act = trajectory.acts.shape\n",
    "            assert real_act == exp_act, f\"expected shape {exp_act}, got {real_act}\"\n",
    "            exp_rew = (n_steps,)\n",
    "            real_rew = trajectory.rews.shape\n",
    "            assert real_rew == exp_rew, f\"expected shape {exp_rew}, got {real_rew}\"\n",
    "\n",
    "        return trajectories\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from imitation.util.util import make_vec_env\n",
    "import kris_envs\n",
    "import rclpy\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rclpy.init()\n",
    "env = make_vec_env(\n",
    "        \"kris_envs/KrisEnv-v0\",\n",
    "        rng=np.random.default_rng(),\n",
    "        n_envs=1,\n",
    "        post_wrappers=[lambda env, _: RolloutInfoWrapper(env)],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "file_path=\"/home/foxy_user/foxy_ws/src/gail_navigation/GailNavigationNetwork/data/traj2.hdf5\"\n",
    "trajectory=TrajFromFile(file_path)\n",
    "print(\"Traj object created with lenth\",trajectory.len)\n",
    "trajs=trajectory.generate_trajectories(env,trajectory.len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_venv",
   "language": "python",
   "name": "thesis_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
