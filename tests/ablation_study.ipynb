{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9ddb65c-b5d2-46ec-9d42-4de95c72d93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torchvision\n",
    "# import numpy as np\n",
    "# from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "# resnet_model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "# The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. \n",
    "# You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
    "# print(resnet_model)\n",
    "# # This method remove the layers upto to the convolution feature maps\n",
    "# modules = list(resnet_model.children())[:-2]\n",
    "# backbone = torch.nn.Sequential(*modules)\n",
    "# print(backbone)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661c7a3a",
   "metadata": {},
   "source": [
    "## Version 1 of the NaviNet\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "10c7703e-fa92-4a1c-b40d-e3877dbeb846",
   "metadata": {},
   "source": [
    "import torch\n",
    "from torch.nn import Conv2d, Linear\n",
    "from torch.nn import Module\n",
    "from torchvision.models import resnet18, ResNet18_Weights,efficientnet_b1,EfficientNet_B1_Weights\n",
    "import h5py\n",
    "import numpy as np\n",
    "from GailNavigationNetwork.model import NaviNet\n",
    "\n",
    "class RGBNet(Module):\n",
    "    def __init__(self,ablation_depth=2):\n",
    "        super().__init__()\n",
    "        resnet_model = efficientnet_b1(weights=EfficientNet_B1_Weights.IMAGENET1K_V2)\n",
    "        modules = list(resnet_model.children())[:-ablation_depth]\n",
    "        self.backbone = torch.nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return x\n",
    "\n",
    "class DepthNet(Module):\n",
    "    def __init__(self,dims=(1,240,320)):\n",
    "        super(DepthNet, self).__init__()\n",
    "        self.conv1 = Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = Linear(dims[0] * dims[1], 128)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        return x\n",
    "\n",
    "class NaviNet(Module):\n",
    "    '''\n",
    "    A deeplearning architecture for local navigation planning\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 image_dims=(240,320),\n",
    "                 goal_dims=7):\n",
    "         super(NaviNet, self).__init__()\n",
    "         self.depth_net = DepthNet(dims=(image_dims[0],image_dims[1],1))\n",
    "         self.rgb_net = RGBNet(ablation_depth=2)\n",
    "         self.fc_goal_pose = Linear(goal_dims, 128) \n",
    "\n",
    "    def forward(self, rgb_image, depth_image, goal_pose):\n",
    "        rgb_features = self.rgb_net(rgb_image).squeeze()\n",
    "        print(f\"rgb_features: {rgb_features.shape}\")\n",
    "        depth_features = self.depth_net(depth_image)\n",
    "        print(f\"depth_features: {depth_features.shape}\")\n",
    "        goal_pose = torch.relu(self.fc_goal_pose(goal_pose))\n",
    "        print(f\"goal_pose: {goal_pose.shape}\")\n",
    "        \n",
    "        # Concatenate features\n",
    "        # concatenated_features = torch.cat((rgb_features.squeeze(), depth_features.unsqueeze(0), goal_pose.unsqueeze(0)), dim=1)\n",
    "        \n",
    "        return rgb_features, depth_features, goal_pose"
   ]
  },
  {
   "cell_type": "raw",
   "id": "33341a7f",
   "metadata": {},
   "source": [
    "file_path=\"/home/foxy_user/foxy_ws/src/gail_navigation/GailNavigationNetwork/data/traj1.hdf5\"\n",
    "read_file= h5py.File(file_path, \"r\")\n",
    "rgb_image=np.expand_dims((read_file['images']['rgb_data'][0]).reshape(3,240,320),axis=0)\n",
    "rgb_image = torch.tensor(rgb_image,dtype=torch.float32) # Example shape, adjust as needed\n",
    "depth_image=np.expand_dims((read_file['images']['depth_data'][0]).reshape(240,320),axis=0)\n",
    "depth_image = torch.tensor(depth_image,dtype=torch.float32)  # Example shape, adjust as needed\n",
    "goal_pose=np.expand_dims(read_file['kris_dynamics']['odom_data']['target_vector'][0],axis=0)\n",
    "goal_pose = torch.tensor(goal_pose,dtype=torch.float32)  # Example shape\n",
    "actions=np.expand_dims(read_file['kris_dynamics']['odom_data']['odom_data_filtered'][0],axis=0)\n",
    "actions= torch.tensor(actions,dtype=torch.float32)  # Example shape\n",
    "print(f\"rgb_image: {rgb_image.shape} and depth_image: {depth_image.shape} \\\n",
    "      goal_pose: {goal_pose.shape} and actions: {actions.shape} \\n \\n\")\n",
    "model= NaviNet()\n",
    "rgb_features,depth_features,goal_features = model(rgb_image, depth_image, goal_pose)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cca20fc",
   "metadata": {},
   "source": [
    "## Version 2 of NaviNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbac277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "27e6c737",
   "metadata": {},
   "source": [
    "import torch\n",
    "from torch.nn import Conv2d, Linear,Parameter\n",
    "from torch.nn import Module\n",
    "from torchvision.models import efficientnet_b1,EfficientNet_B1_Weights\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class RGBNet(Module):\n",
    "    def __init__(self,ablation_depth=2):\n",
    "        super().__init__()\n",
    "        resnet_model = efficientnet_b1(weights=EfficientNet_B1_Weights.IMAGENET1K_V2)\n",
    "        modules = list(resnet_model.children())[:-ablation_depth]\n",
    "        self.backbone = torch.nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return x\n",
    "\n",
    "class DepthNet(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.filter = Conv2d(in_channels=1, out_channels=2, kernel_size=3, \n",
    "                             stride=1, padding=0, bias=False)\n",
    "\n",
    "        Gx = torch.tensor([[2.0, 0.0, -2.0], [4.0, 0.0, -4.0], [2.0, 0.0, -2.0]])\n",
    "        Gy = torch.tensor([[2.0, 4.0, 2.0], [0.0, 0.0, 0.0], [-2.0, -4.0, -2.0]])\n",
    "        G = torch.cat([Gx.unsqueeze(0), Gy.unsqueeze(0)], 0)\n",
    "        G = G.unsqueeze(1)\n",
    "        self.filter.weight = Parameter(G, requires_grad=False)\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.filter(img)\n",
    "        x = torch.mul(x, x)\n",
    "        x = torch.sum(x, dim=1, keepdim=True)\n",
    "        x = torch.sqrt(x)\n",
    "        return x\n",
    "\n",
    "class NaviNet(Module):\n",
    "    '''\n",
    "    A deeplearning architecture for local navigation planning\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 image_dims=(240,320),\n",
    "                 goal_dims=7):\n",
    "         super(NaviNet, self).__init__()\n",
    "         self.depth_net = DepthNet()\n",
    "         self.rgb_net = RGBNet(ablation_depth=2)\n",
    "         self.fc_goal_pose = Linear(goal_dims, 128) \n",
    "\n",
    "    def forward(self, rgb_image, depth_image, goal_pose):\n",
    "        rgb_features = self.rgb_net(rgb_image)\n",
    "        print(f\"rgb_features: {rgb_features.shape}\")\n",
    "        depth_features = self.depth_net(depth_image)\n",
    "        print(f\"depth_features: {depth_features.shape}\")\n",
    "        goal_pose = torch.relu(self.fc_goal_pose(goal_pose))\n",
    "        print(f\"goal_pose: {goal_pose.shape}\")\n",
    "        \n",
    "        # Concatenate features\n",
    "        # concatenated_features = torch.cat((rgb_features.squeeze(), depth_features, goal_pose), dim=1)\n",
    "        \n",
    "        # return concatenated_features\n",
    "        return rgb_features, depth_features, goal_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619e2dd9-34f6-49e6-bcf5-21da6393e197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "2e7191b8",
   "metadata": {},
   "source": [
    "\n",
    "file_path=\"/home/foxy_user/foxy_ws/src/gail_navigation/GailNavigationNetwork/data/traj1.hdf5\"\n",
    "read_file= h5py.File(file_path, \"r\")\n",
    "rgb_image=np_img_to_tensor(read_file['images']['rgb_data'][0])\n",
    "depth_image=np_img_to_tensor(read_file['images']['depth_data'][0])\n",
    "goal_pose=np.expand_dims(read_file['kris_dynamics']['odom_data']['target_vector'][0],axis=0)\n",
    "goal_pose = torch.tensor(goal_pose,dtype=torch.float32)  # Example shape\n",
    "actions=np.expand_dims(read_file['kris_dynamics']['odom_data']['odom_data_filtered'][0],axis=0)\n",
    "actions= torch.tensor(actions,dtype=torch.float32)  # Example shape\n",
    "print(f\"rgb_image: {rgb_image.shape} and depth_image: {depth_image.shape} \\\n",
    "      goal_pose: {goal_pose.shape} and actions: {actions.shape} \\n \\n\")\n",
    "model= NaviNet()\n",
    "_,output,_ = model(rgb_image, depth_image, goal_pose)\n",
    "print(output.shape)  # Check the shape of the output\n",
    "plt.imshow(output, cmap=\"gray\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4301cdf",
   "metadata": {},
   "source": [
    "## Version 3 of Navinet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cc799f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Conv2d, Linear,Parameter\n",
    "from torch.nn import Module\n",
    "from torchvision.models import efficientnet_b1,EfficientNet_B1_Weights\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class RGBNet(Module):\n",
    "    def __init__(self,ablation_depth=2):\n",
    "        super().__init__()\n",
    "        resnet_model = efficientnet_b1(weights=EfficientNet_B1_Weights.IMAGENET1K_V2)\n",
    "        modules = list(resnet_model.children())[:-ablation_depth]\n",
    "        self.backbone = torch.nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return x\n",
    "\n",
    "class DepthNet(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.filter = Conv2d(in_channels=1, out_channels=2, kernel_size=3, \n",
    "                             stride=1, padding=0, bias=False)\n",
    "\n",
    "        Gx = torch.tensor([[2.0, 0.0, -2.0], [4.0, 0.0, -4.0], [2.0, 0.0, -2.0]])\n",
    "        Gy = torch.tensor([[2.0, 4.0, 2.0], [0.0, 0.0, 0.0], [-2.0, -4.0, -2.0]])\n",
    "        G = torch.cat([Gx.unsqueeze(0), Gy.unsqueeze(0)], 0)\n",
    "        G = G.unsqueeze(1)\n",
    "        self.filter.weight = Parameter(G, requires_grad=False)\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.filter(img)\n",
    "        x = torch.mul(x, x)\n",
    "        x = torch.sum(x, dim=1, keepdim=True)\n",
    "        x = torch.sqrt(x)\n",
    "        return x\n",
    "\n",
    "class NaviNet(Module):\n",
    "    '''\n",
    "    A deeplearning architecture for local navigation planning\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 image_dims=(240,320),\n",
    "                 goal_dims=7):\n",
    "         super(NaviNet, self).__init__()\n",
    "         self.depth_net = DepthNet()\n",
    "         self.rgb_net = RGBNet(ablation_depth=2)\n",
    "         self.fc_goal_pose = Linear(goal_dims, 128) \n",
    "\n",
    "    def forward(self, rgb_image, depth_image):\n",
    "        rgb_features = self.rgb_net(rgb_image).squeeze()\n",
    "        depth_features = self.depth_net(depth_image)\n",
    "        print(f\"rgb_features: {rgb_features.shape}\")\n",
    "        print(f\"depth_features: {depth_features.shape}\")\n",
    "       \n",
    "        \n",
    "        # Concatenate features\n",
    "        # concatenated_features = torch.cat((rgb_features.squeeze(), depth_features, goal_pose), dim=1)\n",
    "        \n",
    "        # return concatenated_features\n",
    "        return rgb_features, depth_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9118d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rgb_image: torch.Size([1, 3, 240, 320]) and depth_image: torch.Size([1, 240, 320])       goal_pose: torch.Size([1, 7]) and actions: torch.Size([1, 7]) \n",
      " \n",
      "\n",
      "rgb_features: torch.Size([1280, 8, 10])\n",
      "depth_features: torch.Size([2, 1, 318])\n"
     ]
    }
   ],
   "source": [
    "file_path=\"/home/foxy_user/foxy_ws/src/gail_navigation/GailNavigationNetwork/data/traj1.hdf5\"\n",
    "read_file= h5py.File(file_path, \"r\")\n",
    "rgb_image=np.expand_dims((read_file['images']['rgb_data'][0]).reshape(3,240,320),axis=0)\n",
    "rgb_image = torch.tensor(rgb_image,dtype=torch.float32) # Example shape, adjust as needed\n",
    "depth_image=np.expand_dims((read_file['images']['depth_data'][0]).reshape(240,320),axis=0)\n",
    "# depth_image=(read_file['images']['depth_data'][0])\n",
    "\n",
    "depth_image = torch.tensor(depth_image,dtype=torch.float32)  # Example shape, adjust as needed\n",
    "goal_pose=np.expand_dims(read_file['kris_dynamics']['odom_data']['target_vector'][0],axis=0)\n",
    "goal_pose = torch.tensor(goal_pose,dtype=torch.float32)  # Example shape\n",
    "actions=np.expand_dims(read_file['kris_dynamics']['odom_data']['odom_data_filtered'][0],axis=0)\n",
    "actions= torch.tensor(actions,dtype=torch.float32)  # Example shape\n",
    "print(f\"rgb_image: {rgb_image.shape} and depth_image: {depth_image.shape} \\\n",
    "      goal_pose: {goal_pose.shape} and actions: {actions.shape} \\n \\n\")\n",
    "model= NaviNet()\n",
    "rgb_features,depth_features = model(rgb_image, depth_image)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7795b470",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_venv",
   "language": "python",
   "name": "thesis_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
